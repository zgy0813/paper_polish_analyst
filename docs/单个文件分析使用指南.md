# 单个PDF文件分析使用指南

## 概述

本指南介绍如何使用论文风格分析与润色系统进行单个PDF文件分析。与批量分析不同，单个文件分析专注于逐一处理每个PDF文件，提供更精细的控制和监控。

## 功能特点

### ✅ 主要特性
- **独立分析**：不依赖批量分析，可以单独运行
- **进度跟踪**：实时监控分析进度和状态
- **断点续传**：支持从上次中断的地方继续分析
- **灵活控制**：可以限制分析文件数量
- **错误处理**：详细的错误日志和失败文件记录
- **统计分析**：提供完整的分析统计信息

### 🔧 技术特点
- **分层温度配置**：不同任务使用最适合的AI参数
- **健壮JSON解析**：多层级解析策略确保成功
- **智能过滤**：自动跳过已分析的文件
- **详细日志**：记录所有分析过程和错误

## 使用方法

### 1. 命令行使用

#### 基本用法
```bash
# 分析所有PDF文件
python main.py analyze-individual

# 限制分析数量
python main.py analyze-individual --max-papers 10

# 从上次中断的地方继续
python main.py analyze-individual --resume

# 显示实时进度
python main.py analyze-individual --progress
```

#### 参数说明
- `--max-papers, -m`: 最大分析论文数量，不指定则分析所有
- `--resume`: 从上次中断的地方继续分析
- `--progress`: 显示实时分析进度

### 2. 编程接口使用

#### 基本示例
```python
from src.analysis.layered_analyzer import LayeredAnalyzer

# 创建分析器
analyzer = LayeredAnalyzer()

# 分析所有文件
result = analyzer.analyze_all_individual_papers()

# 检查结果
if 'error' in result:
    print(f"分析失败: {result['error']}")
else:
    print(f"分析完成: {result['successful_papers']}/{result['total_papers']}")
```

#### 高级用法
```python
# 限制分析数量
result = analyzer.analyze_all_individual_papers(max_papers=5)

# 从上次中断的地方继续
result = analyzer.analyze_all_individual_papers(resume=True)

# 获取分析进度
progress = analyzer.get_analysis_progress()
print(f"进度: {progress['completed_files']}/{progress['total_files']}")
```

## 分析流程

### 1. 文件发现
- 扫描 `data/extracted/` 目录中的所有 `.txt` 文件
- 按文件名排序，确保分析顺序一致

### 2. 文件过滤
- 检查 `data/individual_reports/` 目录中是否已存在分析报告
- 跳过已成功分析的文件
- 重新分析有错误的文件

### 3. 逐个分析
- 使用NLP工具进行基础分析
- 调用AI进行深度分析
- 保存分析结果到JSON文件

### 4. 进度跟踪
- 实时更新分析进度
- 记录成功和失败的文件数量
- 提供当前正在分析的文件信息

### 5. 结果汇总
- 生成详细的分析摘要
- 统计文本长度和词数信息
- 记录失败的文件列表

## 输出文件

### 1. 个体分析报告
位置：`data/individual_reports/{paper_id}.json`

```json
{
  "paper_id": "example_paper",
  "analysis_date": "2025-01-14T10:30:00",
  "nlp_analysis": {
    "sentence_structure": {...},
    "vocabulary": {...},
    "paragraph_structure": {...},
    "academic_expression": {...}
  },
  "gpt_analysis": {
    "writing_patterns": {...},
    "style_characteristics": {...}
  },
  "text_length": 50000,
  "word_count": 8000
}
```

### 2. 分析摘要
位置：`data/individual_analysis_summary.json`

```json
{
  "analysis_type": "individual_only",
  "analysis_date": "2025-01-14T10:30:00",
  "total_papers": 82,
  "successful_papers": 80,
  "failed_papers": 2,
  "success_rate": 0.975,
  "text_statistics": {
    "avg_text_length": 45000,
    "min_text_length": 20000,
    "max_text_length": 80000,
    "avg_word_count": 7200,
    "min_word_count": 3200,
    "max_word_count": 12800
  },
  "failed_papers": ["paper1", "paper2"],
  "progress": {
    "total_files": 82,
    "completed_files": 80,
    "failed_files": 2,
    "current_file": null
  }
}
```

### 3. 错误日志
位置：`logs/json_parse_errors.json`

```json
{
  "timestamp": "2025-01-14T10:30:00",
  "task_type": "individual_analysis",
  "error": "JSON解析错误详情",
  "response_length": 1500,
  "response_preview": "响应内容预览..."
}
```

## 配置参数

### 1. 温度配置
```python
temperature_config = {
    'individual_analysis': 0.4,    # 单篇分析需要创造性
    'batch_summary': 0.3,          # 批次汇总需要平衡
    'global_integration': 0.2,     # 全局整合需要稳定
    # ... 其他任务类型
}
```

### 2. Token配置
```python
max_tokens_config = {
    'individual_analysis': 15000,    # 单篇分析
    'batch_summary': 20000,          # 批次汇总
    'global_integration': 25000,     # 全局整合
    # ... 其他任务类型
}
```

## 监控和调试

### 1. 实时进度监控
```python
import time

analyzer = LayeredAnalyzer()

# 启动分析（在后台线程中）
import threading
analysis_thread = threading.Thread(
    target=analyzer.analyze_all_individual_papers
)
analysis_thread.start()

# 监控进度
while analysis_thread.is_alive():
    progress = analyzer.get_analysis_progress()
    print(f"进度: {progress['completed_files']}/{progress['total_files']}")
    time.sleep(5)
```

### 2. 错误调试
```python
# 检查失败的文件
result = analyzer.analyze_all_individual_papers()
if result['failed_papers']:
    print("失败的文件:")
    for paper_id in result['failed_papers']:
        print(f"  - {paper_id}")
        
        # 检查具体的错误信息
        report_file = f"data/individual_reports/{paper_id}.json"
        with open(report_file, 'r') as f:
            report = json.load(f)
            if 'error' in report:
                print(f"    错误: {report['error']}")
```

### 3. 日志分析
```bash
# 查看分析日志
tail -f logs/app_*.log

# 查看JSON解析错误
cat logs/json_parse_errors.json | jq '.'
```

## 性能优化

### 1. 批量大小调整
- 对于大量文件，建议分批处理
- 每批处理10-20个文件，避免内存占用过高

### 2. 错误处理
- 定期检查错误日志
- 及时处理解析失败的文件
- 使用resume功能避免重复分析

### 3. 资源监控
- 监控内存使用情况
- 注意API调用频率限制
- 定期清理临时文件

## 常见问题

### Q1: 分析过程中断怎么办？
A: 使用 `--resume` 参数从上次中断的地方继续分析。

### Q2: 如何只分析特定的文件？
A: 目前不支持直接指定文件，但可以通过修改 `data/extracted/` 目录中的文件来控制。

### Q3: 分析速度很慢怎么办？
A: 检查网络连接和API配置，考虑使用更快的模型或调整参数。

### Q4: 如何查看分析进度？
A: 使用 `--progress` 参数或编程接口的 `get_analysis_progress()` 方法。

### Q5: 分析结果在哪里？
A: 个体分析结果在 `data/individual_reports/` 目录，摘要在 `data/individual_analysis_summary.json`。

## 最佳实践

1. **定期备份**：分析前备份重要数据
2. **分批处理**：大量文件时分批分析
3. **监控进度**：使用进度监控功能
4. **错误处理**：及时处理分析失败的文件
5. **日志管理**：定期清理和归档日志文件

## 总结

单个PDF文件分析功能提供了灵活、可控的论文分析方式，特别适合需要精细控制分析过程的场景。通过合理的配置和使用，可以高效地完成大量论文的分析工作。
