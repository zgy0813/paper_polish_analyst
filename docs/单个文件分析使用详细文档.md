# 单个PDF文件分析使用指南

## 概述

本指南介绍如何使用论文风格分析与润色系统进行单个PDF文件分析。与批量分析不同，单个文件分析专注于逐一处理每个PDF文件，提供更精细的控制和监控。

## 功能特点

### ✅ 主要特性
- **独立分析**：不依赖批量分析，可以单独运行
- **进度跟踪**：实时监控分析进度和状态
- **断点续传**：支持从上次中断的地方继续分析
- **灵活控制**：可以限制分析文件数量
- **错误处理**：详细的错误日志和失败文件记录
- **统计分析**：提供完整的分析统计信息

### 🔧 技术特点
- **分层温度配置**：不同任务使用最适合的AI参数
- **健壮JSON解析**：多层级解析策略确保成功
- **智能过滤**：自动跳过已分析的文件
- **详细日志**：记录所有分析过程和错误

## 使用方法

### 1. 命令行使用

#### 基本用法
```bash
# 分析所有PDF文件
python main.py analyze-individual

# 限制分析数量
python main.py analyze-individual --max-papers 10

# 从上次中断的地方继续
python main.py analyze-individual --resume

# 显示实时进度
python main.py analyze-individual --progress
```

#### 参数说明
- `--max-papers, -m`: 最大分析论文数量，不指定则分析所有
- `--resume`: 从上次中断的地方继续分析
- `--progress`: 显示实时分析进度

### 2. 编程接口使用

#### 基本示例
```python
from src.analysis.layered_analyzer import LayeredAnalyzer

# 创建分析器
analyzer = LayeredAnalyzer()

# 分析所有文件
result = analyzer.analyze_all_individual_papers()

# 检查结果
if 'error' in result:
    print(f"分析失败: {result['error']}")
else:
    print(f"分析完成: {result['successful_papers']}/{result['total_papers']}")
```

#### 高级用法
```python
# 限制分析数量
result = analyzer.analyze_all_individual_papers(max_papers=5)

# 从上次中断的地方继续
result = analyzer.analyze_all_individual_papers(resume=True)

# 获取分析进度
progress = analyzer.get_analysis_progress()
print(f"进度: {progress['completed_files']}/{progress['total_files']}")
```

## 分析流程

### 1. 文件发现
- 扫描 `data/extracted/` 目录中的所有 `.txt` 文件
- 按文件名排序，确保分析顺序一致

### 2. 文件过滤
- 检查 `data/individual_reports/` 目录中是否已存在分析报告
- 跳过已成功分析的文件
- 重新分析有错误的文件

### 3. 逐个分析
- 使用spaCy进行基础NLP分析
- 调用AI进行深度语义分析
- 融合NLP和AI分析结果
- 保存分析结果到JSON文件

### 4. 进度跟踪
- 实时更新分析进度
- 记录成功和失败的文件数量
- 提供当前正在分析的文件信息

### 5. 结果汇总
- 生成详细的分析摘要
- 统计文本长度和词数信息
- 记录失败的文件列表

## 输出文件

### 1. 个体分析报告
位置：`data/individual_reports/{paper_id}.json`

```json
{
  "paper_id": "example_paper",
  "analysis_date": "2025-01-14T10:30:00",
  "nlp_analysis": {
    "sentence_structure": {...},
    "vocabulary": {...},
    "paragraph_structure": {...},
    "academic_expression": {...}
  },
  "gpt_analysis": {
    "writing_patterns": {...},
    "style_characteristics": {...}
  },
  "text_length": 50000,
  "word_count": 8000
}
```

### 2. 分析摘要
位置：`data/individual_analysis_summary.json`

```json
{
  "analysis_type": "individual_only",
  "analysis_date": "2025-01-14T10:30:00",
  "total_papers": 82,
  "successful_papers": 80,
  "failed_papers": 2,
  "success_rate": 0.975,
  "text_statistics": {
    "avg_text_length": 45000,
    "min_text_length": 20000,
    "max_text_length": 80000,
    "avg_word_count": 7200,
    "min_word_count": 3200,
    "max_word_count": 12800
  },
  "failed_papers": ["paper1", "paper2"],
  "progress": {
    "total_files": 82,
    "completed_files": 80,
    "failed_files": 2,
    "current_file": null
  }
}
```

### 3. 错误日志
位置：`logs/json_parse_errors.json`

```json
{
  "timestamp": "2025-01-14T10:30:00",
  "task_type": "individual_analysis",
  "error": "JSON解析错误详情",
  "response_length": 1500,
  "response_preview": "响应内容预览..."
}
```

## 配置参数

### 1. 温度配置
```python
temperature_config = {
    'individual_analysis': 0.4,    # 单篇分析需要创造性
    'batch_summary': 0.3,          # 批次汇总需要平衡
    'global_integration': 0.2,     # 全局整合需要稳定
    # ... 其他任务类型
}
```

### 2. Token配置
```python
max_tokens_config = {
    'individual_analysis': 15000,    # 单篇分析
    'batch_summary': 20000,          # 批次汇总
    'global_integration': 25000,     # 全局整合
    # ... 其他任务类型
}
```

## 监控和调试

### 1. 实时进度监控
```python
import time

analyzer = LayeredAnalyzer()

# 启动分析（在后台线程中）
import threading
analysis_thread = threading.Thread(
    target=analyzer.analyze_all_individual_papers
)
analysis_thread.start()

# 监控进度
while analysis_thread.is_alive():
    progress = analyzer.get_analysis_progress()
    print(f"进度: {progress['completed_files']}/{progress['total_files']}")
    time.sleep(5)
```

### 2. 错误调试
```python
# 检查失败的文件
result = analyzer.analyze_all_individual_papers()
if result['failed_papers']:
    print("失败的文件:")
    for paper_id in result['failed_papers']:
        print(f"  - {paper_id}")
        
        # 检查具体的错误信息
        report_file = f"data/individual_reports/{paper_id}.json"
        with open(report_file, 'r') as f:
            report = json.load(f)
            if 'error' in report:
                print(f"    错误: {report['error']}")
```

### 3. 日志分析
```bash
# 查看分析日志
tail -f logs/app_*.log

# 查看JSON解析错误
cat logs/json_parse_errors.json | jq '.'
```

## 性能优化

### 1. 批量大小调整
- 对于大量文件，建议分批处理
- 每批处理10-20个文件，避免内存占用过高

### 2. 错误处理
- 定期检查错误日志
- 及时处理解析失败的文件
- 使用resume功能避免重复分析

### 3. 资源监控
- 监控内存使用情况
- 注意API调用频率限制
- 定期清理临时文件

## 常见问题

### Q1: 分析过程中断怎么办？
A: 使用 `--resume` 参数从上次中断的地方继续分析。

### Q2: 如何只分析特定的文件？
A: 目前不支持直接指定文件，但可以通过修改 `data/extracted/` 目录中的文件来控制。

### Q3: 分析速度很慢怎么办？
A: 检查网络连接和API配置，考虑使用更快的模型或调整参数。

### Q4: 如何查看分析进度？
A: 使用 `--progress` 参数或编程接口的 `get_analysis_progress()` 方法。

### Q5: 分析结果在哪里？
A: 个体分析结果在 `data/individual_reports/` 目录，摘要在 `data/individual_analysis_summary.json`。

## 最佳实践

1. **定期备份**：分析前备份重要数据
2. **分批处理**：大量文件时分批分析
3. **监控进度**：使用进度监控功能
4. **错误处理**：及时处理分析失败的文件
5. **日志管理**：定期清理和归档日志文件

## spaCy与AI协作分析机制详解

### 🔄 分析流程架构

spaCy与AI在单个文件分析中采用**分层协作**的工作模式：

```
PDF文本 → spaCy预处理 → 特征提取 → AI深度分析 → 结果融合 → 分析报告
```

### 📊 第一层：spaCy预处理与特征提取

#### 1. 文本预处理
```python
# spaCy模型初始化（en_core_web_md，包含685k词向量）
self.nlp = spacy.load("en_core_web_md")

# 文本解析
doc = self.nlp(text)
```

#### 2. 四大核心分析维度

**A. 句式结构分析 (`analyze_sentence_structure`)**
```python
# spaCy提供精确的句子分割
sentences = [sent.text.strip() for sent in doc.sents]

# 使用spaCy的词汇化进行准确分词
sentence_lengths = []
for sent in sentences:
    sent_doc = self.nlp(sent)
    # 只统计字母词汇，过滤标点和数字
    sentence_lengths.append(len([token for token in sent_doc if token.is_alpha]))
```

**关键指标：**
- `total_sentences`: 总句子数
- `avg_sentence_length`: 平均句长（基于spaCy精确分词）
- `compound_sentence_ratio`: 复合句比例（包含and, but, or等连接词）
- `complex_sentence_ratio`: 从句比例（包含because, although, which等从属连词）
- `sentence_length_variance`: 句长变化方差

**B. 词汇特点分析 (`analyze_vocabulary`)**
```python
# spaCy的词汇化处理
words = [token.text for token in doc if token.is_alpha and token.text.lower() not in self.stop_words]

# 学术词汇识别（基于词缀模式）
academic_patterns = [
    r'.*tion$', r'.*sion$', r'.*ment$',  # 名词后缀
    r'^analy.*', r'^investig.*', r'^examin.*',  # 研究动词
    r'^signific.*', r'^substantial.*', r'^considerable.*'  # 程度词
]
```

**关键指标：**
- `total_words`: 总词数（经过spaCy过滤）
- `unique_words`: 唯一词数
- `vocabulary_richness`: 词汇丰富度（Type-Token Ratio）
- `academic_word_ratio`: 学术词汇比例
- `most_common_words`: 高频词统计

**C. 段落结构分析 (`analyze_paragraph_structure`)**
```python
# 段落分割
paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]

# 使用spaCy分析段落长度
paragraph_lengths = []
for para in paragraphs:
    para_doc = self.nlp(para)
    paragraph_lengths.append(len([token for token in para_doc if token.is_alpha]))
```

**关键指标：**
- `total_paragraphs`: 总段落数
- `avg_paragraph_length`: 平均段落长度
- `paragraph_length_variance`: 段落长度变化
- `topic_sentence_analysis`: 主题句特征分析

**D. 学术表达习惯分析 (`analyze_academic_expression`)**
```python
# spaCy的被动语态检测
def _calculate_passive_voice_ratio(self, sentences: List[str]) -> float:
    total_verbs = 0
    passive_verbs = 0
    
    for sentence in sentences:
        sent_doc = self.nlp(sentence)
        for token in sent_doc:
            if token.pos_ == "AUX" and token.dep_ == "auxpass":
                passive_verbs += 1
            elif token.pos_ == "VERB":
                total_verbs += 1
```

**关键指标：**
- `passive_voice_ratio`: 被动语态比例（基于spaCy语法分析）
- `first_person_usage`: 第一人称使用情况
- `qualifier_usage`: 限定词使用情况

#### 3. spaCy高级功能应用

**A. 命名实体识别**
```python
def analyze_advanced_features(self, text: str) -> Dict:
    doc = self.nlp(text)
    
    # 识别学术文本中的实体
    entities = {}
    for ent in doc.ents:
        if ent.label_ not in entities:
            entities[ent.label_] = []
        entities[ent.label_].append(ent.text)
```

**B. 词性标注与依存分析**
```python
# 词性分布统计
pos_counts = {}
for token in doc:
    if token.pos_ not in pos_counts:
        pos_counts[token.pos_] = 0
    pos_counts[token.pos_] += 1

# 依存关系模式
dependency_patterns = {}
for token in doc:
    if token.dep_ not in dependency_patterns:
        dependency_patterns[token.dep_] = 0
    dependency_patterns[token.dep_] += 1
```

**C. 词向量语义相似度**
```python
def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
    doc1 = self.nlp(text1)
    doc2 = self.nlp(text2)
    # 使用spaCy的内置相似度计算（基于685k词向量）
    similarity = doc1.similarity(doc2)
    return float(similarity)
```

**D. 学术关键词提取**
```python
def extract_academic_keywords(self, text: str, top_n: int = 10) -> List[Dict]:
    doc = self.nlp(text)
    word_scores = {}
    
    for token in doc:
        if (token.is_alpha and 
            token.pos_ in ['NOUN', 'ADJ', 'VERB'] and 
            len(token.text) > 3 and 
            token.text.lower() not in self.stop_words):
            
            word = token.text.lower()
            base_score = word_scores.get(word, 0) + 1
            
            # 词性权重
            pos_weight = {'NOUN': 1.5, 'ADJ': 1.2, 'VERB': 1.3}.get(token.pos_, 1.0)
            
            # 学术词汇权重
            academic_weight = 1.5 if any(pattern in word for pattern in 
                ['tion', 'sion', 'ment', 'analy', 'investig', 'examin']) else 1.0
            
            word_scores[word] = base_score * pos_weight * academic_weight
```

### 🤖 第二层：AI深度分析

#### 1. AI分析输入准备
spaCy提取的结构化数据被整合为AI分析的输入：

```python
# NLP分析结果
nlp_analysis = {
    'sentence_structure': self.nlp_utils.analyze_sentence_structure(paper_text),
    'vocabulary': self.nlp_utils.analyze_vocabulary(paper_text),
    'paragraph_structure': self.nlp_utils.analyze_paragraph_structure(paper_text),
    'academic_expression': self.nlp_utils.analyze_academic_expression(paper_text)
}
```

#### 2. AI Prompt设计
AI分析基于详细的prompt模板，要求分析以下维度：

**A. 句式结构特征**
- 平均句子长度（基于spaCy的精确统计）
- 复合句比例（利用spaCy识别的连接词）
- 复杂句模式（基于spaCy的从属连词识别）
- 句长变化（利用spaCy的方差计算）

**B. 词汇特征**
- 学术词汇密度（结合spaCy的词缀识别）
- 专业术语频率
- 动词时态偏好（基于spaCy的词性标注）
- 高频词分析（基于spaCy的词频统计）

**C. 段落组织**
- 平均段落长度（基于spaCy的段落分析）
- 主题句位置（结合spaCy的句子分割）
- 论证逻辑结构

**D. 学术表达习惯**
- 被动语态使用率（基于spaCy的语法分析）
- 第一人称使用（利用spaCy的代词识别）
- 限定词使用（基于spaCy的词汇分析）

#### 3. AI分析输出格式
```json
{
  "sentence_structure": {
    "avg_sentence_length": 0,
    "compound_sentence_ratio": 0.5,
    "complex_sentence_ratio": 0.3,
    "sentence_length_variance": 0
  },
  "vocabulary": {
    "academic_word_ratio": 0.5,
    "professional_terminology_frequency": 0,
    "verb_tense_preference": "past/present/mixed",
    "top_words": ["word1", "word2", ...]
  },
  "paragraph_organization": {
    "avg_paragraph_length": 0,
    "topic_sentence_position": "beginning/middle/mixed",
    "argument_structure": "description"
  },
  "academic_expression": {
    "passive_voice_ratio": 0.0,
    "first_person_usage": 0.0,
    "qualifier_usage": 0.0
  },
  "citation_argument": {
    "citation_format": "APA/MLA/Other",
    "argument_pattern": "description of argument pattern"
  }
}
```

### 🔗 第三层：结果融合与输出

#### 1. 数据整合
```python
combined_analysis = {
    'paper_id': paper_id,
    'analysis_date': datetime.now().isoformat(),
    'nlp_analysis': nlp_analysis,      # spaCy提取的量化数据
    'gpt_analysis': gpt_analysis,      # AI深度分析结果
    'text_length': len(paper_text),
    'word_count': len(paper_text.split())
}
```

#### 2. 分析报告结构
```json
{
  "paper_id": "example_paper",
  "analysis_date": "2025-01-14T10:30:00",
  "nlp_analysis": {
    "sentence_structure": {
      "total_sentences": 150,
      "avg_sentence_length": 18.5,
      "compound_sentence_ratio": 0.35,
      "complex_sentence_ratio": 0.42,
      "sentence_length_variance": 45.2
    },
    "vocabulary": {
      "total_words": 2850,
      "unique_words": 680,
      "vocabulary_richness": 0.238,
      "academic_word_ratio": 0.156,
      "most_common_words": {
        "research": 45,
        "study": 38,
        "analysis": 32
      }
    },
    "paragraph_structure": {
      "total_paragraphs": 25,
      "avg_paragraph_length": 114.0,
      "paragraph_length_variance": 156.8
    },
    "academic_expression": {
      "passive_voice_ratio": 0.23,
      "first_person_usage": {
        "first_person_count": 3,
        "first_person_ratio": 0.001
      },
      "qualifier_usage": {
        "qualifier_count": 12,
        "qualifier_ratio": 0.004
      }
    }
  },
  "gpt_analysis": {
    "sentence_structure": {
      "avg_sentence_length": 18.5,
      "compound_sentence_ratio": 0.35,
      "complex_sentence_ratio": 0.42
    },
    "vocabulary": {
      "academic_word_ratio": 0.156,
      "professional_terminology_frequency": 0.089,
      "verb_tense_preference": "mixed",
      "top_words": ["research", "study", "analysis", "methodology"]
    },
    "paragraph_organization": {
      "avg_paragraph_length": 114.0,
      "topic_sentence_position": "beginning",
      "argument_structure": "linear progression with supporting evidence"
    },
    "academic_expression": {
      "passive_voice_ratio": 0.23,
      "first_person_usage": 0.001,
      "qualifier_usage": 0.004
    },
    "citation_argument": {
      "citation_format": "APA",
      "argument_pattern": "evidence-based with systematic presentation"
    }
  },
  "text_length": 45000,
  "word_count": 2850
}
```

### 🎯 spaCy与AI协作的优势

#### 1. 数据互补性
- **spaCy提供精确的量化数据**：句长、词频、语法结构等客观指标
- **AI提供深度语义分析**：写作模式、论证逻辑、风格特征等主观判断

#### 2. 分析层次性
- **基础层（spaCy）**：语法、词汇、句法结构的统计特征
- **语义层（AI）**：写作意图、论证策略、表达风格的分析

#### 3. 结果验证性
- **交叉验证**：spaCy的客观数据与AI的主观分析相互验证
- **一致性检查**：确保分析结果的逻辑一致性和可靠性

#### 4. 处理效率性
- **并行处理**：spaCy的快速预处理为AI分析提供结构化输入
- **分层优化**：不同层次使用最适合的分析工具和参数

### 📈 实际应用效果

#### 分析精度提升
- **句法分析精度**：spaCy的en_core_web_md模型提供95%+的词性标注准确率
- **语义理解深度**：AI模型基于spaCy的结构化数据提供更深层的语义分析
- **风格识别准确性**：结合客观统计和主观判断，提高风格模式识别的准确性

#### 处理效率优化
- **预处理加速**：spaCy的Cython实现提供2-3倍的处理速度提升
- **内存优化**：spaCy的流式处理减少内存占用
- **批处理支持**：支持大规模文本的批量分析

#### 结果可解释性
- **量化指标**：spaCy提供可验证的统计指标
- **定性分析**：AI提供可理解的风格描述
- **证据支持**：每个分析结论都有具体的数据支撑

这种spaCy与AI的分层协作模式，既保证了分析的客观性和准确性，又提供了深度和广度，是单个文件分析功能的核心技术优势。

## 总结

单个PDF文件分析功能提供了灵活、可控的论文分析方式，特别适合需要精细控制分析过程的场景。通过spaCy与AI的深度协作，系统能够提供既精确又全面的文本分析，为学术写作提供科学的数据支撑。通过合理的配置和使用，可以高效地完成大量论文的分析工作。
