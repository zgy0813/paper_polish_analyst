# å•ä¸ªPDFæ–‡ä»¶åˆ†æä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨è®ºæ–‡é£æ ¼åˆ†æä¸æ¶¦è‰²ç³»ç»Ÿè¿›è¡Œå•ä¸ªPDFæ–‡ä»¶åˆ†æã€‚ä¸æ‰¹é‡åˆ†æä¸åŒï¼Œå•ä¸ªæ–‡ä»¶åˆ†æä¸“æ³¨äºé€ä¸€å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶ï¼Œæä¾›æ›´ç²¾ç»†çš„æ§åˆ¶å’Œç›‘æ§ã€‚

## åŠŸèƒ½ç‰¹ç‚¹

### âœ… ä¸»è¦ç‰¹æ€§
- **ç‹¬ç«‹åˆ†æ**ï¼šä¸ä¾èµ–æ‰¹é‡åˆ†æï¼Œå¯ä»¥å•ç‹¬è¿è¡Œ
- **è¿›åº¦è·Ÿè¸ª**ï¼šå®æ—¶ç›‘æ§åˆ†æè¿›åº¦å’ŒçŠ¶æ€
- **æ–­ç‚¹ç»­ä¼ **ï¼šæ”¯æŒä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­åˆ†æ
- **çµæ´»æ§åˆ¶**ï¼šå¯ä»¥é™åˆ¶åˆ†ææ–‡ä»¶æ•°é‡
- **é”™è¯¯å¤„ç†**ï¼šè¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œå¤±è´¥æ–‡ä»¶è®°å½•
- **ç»Ÿè®¡åˆ†æ**ï¼šæä¾›å®Œæ•´çš„åˆ†æç»Ÿè®¡ä¿¡æ¯

### ğŸ”§ æŠ€æœ¯ç‰¹ç‚¹
- **åˆ†å±‚æ¸©åº¦é…ç½®**ï¼šä¸åŒä»»åŠ¡ä½¿ç”¨æœ€é€‚åˆçš„AIå‚æ•°
- **å¥å£®JSONè§£æ**ï¼šå¤šå±‚çº§è§£æç­–ç•¥ç¡®ä¿æˆåŠŸ
- **æ™ºèƒ½è¿‡æ»¤**ï¼šè‡ªåŠ¨è·³è¿‡å·²åˆ†æçš„æ–‡ä»¶
- **è¯¦ç»†æ—¥å¿—**ï¼šè®°å½•æ‰€æœ‰åˆ†æè¿‡ç¨‹å’Œé”™è¯¯

## ä½¿ç”¨æ–¹æ³•

### 1. å‘½ä»¤è¡Œä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•
```bash
# åˆ†ææ‰€æœ‰PDFæ–‡ä»¶
python main.py analyze-individual

# é™åˆ¶åˆ†ææ•°é‡
python main.py analyze-individual --max-papers 10

# ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­
python main.py analyze-individual --resume

# æ˜¾ç¤ºå®æ—¶è¿›åº¦
python main.py analyze-individual --progress
```

#### å‚æ•°è¯´æ˜
- `--max-papers, -m`: æœ€å¤§åˆ†æè®ºæ–‡æ•°é‡ï¼Œä¸æŒ‡å®šåˆ™åˆ†ææ‰€æœ‰
- `--resume`: ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­åˆ†æ
- `--progress`: æ˜¾ç¤ºå®æ—¶åˆ†æè¿›åº¦

### 2. ç¼–ç¨‹æ¥å£ä½¿ç”¨

#### åŸºæœ¬ç¤ºä¾‹
```python
from src.analysis.layered_analyzer import LayeredAnalyzer

# åˆ›å»ºåˆ†æå™¨
analyzer = LayeredAnalyzer()

# åˆ†ææ‰€æœ‰æ–‡ä»¶
result = analyzer.analyze_all_individual_papers()

# æ£€æŸ¥ç»“æœ
if 'error' in result:
    print(f"åˆ†æå¤±è´¥: {result['error']}")
else:
    print(f"åˆ†æå®Œæˆ: {result['successful_papers']}/{result['total_papers']}")
```

#### é«˜çº§ç”¨æ³•
```python
# é™åˆ¶åˆ†ææ•°é‡
result = analyzer.analyze_all_individual_papers(max_papers=5)

# ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­
result = analyzer.analyze_all_individual_papers(resume=True)

# è·å–åˆ†æè¿›åº¦
progress = analyzer.get_analysis_progress()
print(f"è¿›åº¦: {progress['completed_files']}/{progress['total_files']}")
```

## åˆ†ææµç¨‹

### 1. æ–‡ä»¶å‘ç°
- æ‰«æ `data/extracted/` ç›®å½•ä¸­çš„æ‰€æœ‰ `.txt` æ–‡ä»¶
- æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿åˆ†æé¡ºåºä¸€è‡´

### 2. æ–‡ä»¶è¿‡æ»¤
- æ£€æŸ¥ `data/individual_reports/` ç›®å½•ä¸­æ˜¯å¦å·²å­˜åœ¨åˆ†ææŠ¥å‘Š
- è·³è¿‡å·²æˆåŠŸåˆ†æçš„æ–‡ä»¶
- é‡æ–°åˆ†ææœ‰é”™è¯¯çš„æ–‡ä»¶

### 3. é€ä¸ªåˆ†æ
- ä½¿ç”¨spaCyè¿›è¡ŒåŸºç¡€NLPåˆ†æ
- è°ƒç”¨AIè¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æ
- èåˆNLPå’ŒAIåˆ†æç»“æœ
- ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶

### 4. è¿›åº¦è·Ÿè¸ª
- å®æ—¶æ›´æ–°åˆ†æè¿›åº¦
- è®°å½•æˆåŠŸå’Œå¤±è´¥çš„æ–‡ä»¶æ•°é‡
- æä¾›å½“å‰æ­£åœ¨åˆ†æçš„æ–‡ä»¶ä¿¡æ¯

### 5. ç»“æœæ±‡æ€»
- ç”Ÿæˆè¯¦ç»†çš„åˆ†ææ‘˜è¦
- ç»Ÿè®¡æ–‡æœ¬é•¿åº¦å’Œè¯æ•°ä¿¡æ¯
- è®°å½•å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨

## è¾“å‡ºæ–‡ä»¶

### 1. ä¸ªä½“åˆ†ææŠ¥å‘Š
ä½ç½®ï¼š`data/individual_reports/{paper_id}.json`

```json
{
  "paper_id": "example_paper",
  "analysis_date": "2025-01-14T10:30:00",
  "nlp_analysis": {
    "sentence_structure": {...},
    "vocabulary": {...},
    "paragraph_structure": {...},
    "academic_expression": {...}
  },
  "gpt_analysis": {
    "writing_patterns": {...},
    "style_characteristics": {...}
  },
  "text_length": 50000,
  "word_count": 8000
}
```

### 2. åˆ†ææ‘˜è¦
ä½ç½®ï¼š`data/individual_analysis_summary.json`

```json
{
  "analysis_type": "individual_only",
  "analysis_date": "2025-01-14T10:30:00",
  "total_papers": 82,
  "successful_papers": 80,
  "failed_papers": 2,
  "success_rate": 0.975,
  "text_statistics": {
    "avg_text_length": 45000,
    "min_text_length": 20000,
    "max_text_length": 80000,
    "avg_word_count": 7200,
    "min_word_count": 3200,
    "max_word_count": 12800
  },
  "failed_papers": ["paper1", "paper2"],
  "progress": {
    "total_files": 82,
    "completed_files": 80,
    "failed_files": 2,
    "current_file": null
  }
}
```

### 3. é”™è¯¯æ—¥å¿—
ä½ç½®ï¼š`logs/json_parse_errors.json`

```json
{
  "timestamp": "2025-01-14T10:30:00",
  "task_type": "individual_analysis",
  "error": "JSONè§£æé”™è¯¯è¯¦æƒ…",
  "response_length": 1500,
  "response_preview": "å“åº”å†…å®¹é¢„è§ˆ..."
}
```

## é…ç½®å‚æ•°

### 1. æ¸©åº¦é…ç½®
```python
temperature_config = {
    'individual_analysis': 0.4,    # å•ç¯‡åˆ†æéœ€è¦åˆ›é€ æ€§
    'batch_summary': 0.3,          # æ‰¹æ¬¡æ±‡æ€»éœ€è¦å¹³è¡¡
    'global_integration': 0.2,     # å…¨å±€æ•´åˆéœ€è¦ç¨³å®š
    # ... å…¶ä»–ä»»åŠ¡ç±»å‹
}
```

### 2. Tokené…ç½®
```python
max_tokens_config = {
    'individual_analysis': 15000,    # å•ç¯‡åˆ†æ
    'batch_summary': 20000,          # æ‰¹æ¬¡æ±‡æ€»
    'global_integration': 25000,     # å…¨å±€æ•´åˆ
    # ... å…¶ä»–ä»»åŠ¡ç±»å‹
}
```

## ç›‘æ§å’Œè°ƒè¯•

### 1. å®æ—¶è¿›åº¦ç›‘æ§
```python
import time

analyzer = LayeredAnalyzer()

# å¯åŠ¨åˆ†æï¼ˆåœ¨åå°çº¿ç¨‹ä¸­ï¼‰
import threading
analysis_thread = threading.Thread(
    target=analyzer.analyze_all_individual_papers
)
analysis_thread.start()

# ç›‘æ§è¿›åº¦
while analysis_thread.is_alive():
    progress = analyzer.get_analysis_progress()
    print(f"è¿›åº¦: {progress['completed_files']}/{progress['total_files']}")
    time.sleep(5)
```

### 2. é”™è¯¯è°ƒè¯•
```python
# æ£€æŸ¥å¤±è´¥çš„æ–‡ä»¶
result = analyzer.analyze_all_individual_papers()
if result['failed_papers']:
    print("å¤±è´¥çš„æ–‡ä»¶:")
    for paper_id in result['failed_papers']:
        print(f"  - {paper_id}")
        
        # æ£€æŸ¥å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        report_file = f"data/individual_reports/{paper_id}.json"
        with open(report_file, 'r') as f:
            report = json.load(f)
            if 'error' in report:
                print(f"    é”™è¯¯: {report['error']}")
```

### 3. æ—¥å¿—åˆ†æ
```bash
# æŸ¥çœ‹åˆ†ææ—¥å¿—
tail -f logs/app_*.log

# æŸ¥çœ‹JSONè§£æé”™è¯¯
cat logs/json_parse_errors.json | jq '.'
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤§å°è°ƒæ•´
- å¯¹äºå¤§é‡æ–‡ä»¶ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†
- æ¯æ‰¹å¤„ç†10-20ä¸ªæ–‡ä»¶ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜

### 2. é”™è¯¯å¤„ç†
- å®šæœŸæ£€æŸ¥é”™è¯¯æ—¥å¿—
- åŠæ—¶å¤„ç†è§£æå¤±è´¥çš„æ–‡ä»¶
- ä½¿ç”¨resumeåŠŸèƒ½é¿å…é‡å¤åˆ†æ

### 3. èµ„æºç›‘æ§
- ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
- æ³¨æ„APIè°ƒç”¨é¢‘ç‡é™åˆ¶
- å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶

## å¸¸è§é—®é¢˜

### Q1: åˆ†æè¿‡ç¨‹ä¸­æ–­æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨ `--resume` å‚æ•°ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­åˆ†æã€‚

### Q2: å¦‚ä½•åªåˆ†æç‰¹å®šçš„æ–‡ä»¶ï¼Ÿ
A: ç›®å‰ä¸æ”¯æŒç›´æ¥æŒ‡å®šæ–‡ä»¶ï¼Œä½†å¯ä»¥é€šè¿‡ä¿®æ”¹ `data/extracted/` ç›®å½•ä¸­çš„æ–‡ä»¶æ¥æ§åˆ¶ã€‚

### Q3: åˆ†æé€Ÿåº¦å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
A: æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–è°ƒæ•´å‚æ•°ã€‚

### Q4: å¦‚ä½•æŸ¥çœ‹åˆ†æè¿›åº¦ï¼Ÿ
A: ä½¿ç”¨ `--progress` å‚æ•°æˆ–ç¼–ç¨‹æ¥å£çš„ `get_analysis_progress()` æ–¹æ³•ã€‚

### Q5: åˆ†æç»“æœåœ¨å“ªé‡Œï¼Ÿ
A: ä¸ªä½“åˆ†æç»“æœåœ¨ `data/individual_reports/` ç›®å½•ï¼Œæ‘˜è¦åœ¨ `data/individual_analysis_summary.json`ã€‚

## æœ€ä½³å®è·µ

1. **å®šæœŸå¤‡ä»½**ï¼šåˆ†æå‰å¤‡ä»½é‡è¦æ•°æ®
2. **åˆ†æ‰¹å¤„ç†**ï¼šå¤§é‡æ–‡ä»¶æ—¶åˆ†æ‰¹åˆ†æ
3. **ç›‘æ§è¿›åº¦**ï¼šä½¿ç”¨è¿›åº¦ç›‘æ§åŠŸèƒ½
4. **é”™è¯¯å¤„ç†**ï¼šåŠæ—¶å¤„ç†åˆ†æå¤±è´¥çš„æ–‡ä»¶
5. **æ—¥å¿—ç®¡ç†**ï¼šå®šæœŸæ¸…ç†å’Œå½’æ¡£æ—¥å¿—æ–‡ä»¶

## spaCyä¸AIåä½œåˆ†ææœºåˆ¶è¯¦è§£

### ğŸ”„ åˆ†ææµç¨‹æ¶æ„

spaCyä¸AIåœ¨å•ä¸ªæ–‡ä»¶åˆ†æä¸­é‡‡ç”¨**åˆ†å±‚åä½œ**çš„å·¥ä½œæ¨¡å¼ï¼š

```
PDFæ–‡æœ¬ â†’ spaCyé¢„å¤„ç† â†’ ç‰¹å¾æå– â†’ AIæ·±åº¦åˆ†æ â†’ ç»“æœèåˆ â†’ åˆ†ææŠ¥å‘Š
```

### ğŸ“Š ç¬¬ä¸€å±‚ï¼šspaCyé¢„å¤„ç†ä¸ç‰¹å¾æå–

#### 1. æ–‡æœ¬é¢„å¤„ç†
```python
# spaCyæ¨¡å‹åˆå§‹åŒ–ï¼ˆen_core_web_mdï¼ŒåŒ…å«685kè¯å‘é‡ï¼‰
self.nlp = spacy.load("en_core_web_md")

# æ–‡æœ¬è§£æ
doc = self.nlp(text)
```

#### 2. å››å¤§æ ¸å¿ƒåˆ†æç»´åº¦

**A. å¥å¼ç»“æ„åˆ†æ (`analyze_sentence_structure`)**
```python
# spaCyæä¾›ç²¾ç¡®çš„å¥å­åˆ†å‰²
sentences = [sent.text.strip() for sent in doc.sents]

# ä½¿ç”¨spaCyçš„è¯æ±‡åŒ–è¿›è¡Œå‡†ç¡®åˆ†è¯
sentence_lengths = []
for sent in sentences:
    sent_doc = self.nlp(sent)
    # åªç»Ÿè®¡å­—æ¯è¯æ±‡ï¼Œè¿‡æ»¤æ ‡ç‚¹å’Œæ•°å­—
    sentence_lengths.append(len([token for token in sent_doc if token.is_alpha]))
```

**å…³é”®æŒ‡æ ‡ï¼š**
- `total_sentences`: æ€»å¥å­æ•°
- `avg_sentence_length`: å¹³å‡å¥é•¿ï¼ˆåŸºäºspaCyç²¾ç¡®åˆ†è¯ï¼‰
- `compound_sentence_ratio`: å¤åˆå¥æ¯”ä¾‹ï¼ˆåŒ…å«and, but, orç­‰è¿æ¥è¯ï¼‰
- `complex_sentence_ratio`: ä»å¥æ¯”ä¾‹ï¼ˆåŒ…å«because, although, whichç­‰ä»å±è¿è¯ï¼‰
- `sentence_length_variance`: å¥é•¿å˜åŒ–æ–¹å·®

**B. è¯æ±‡ç‰¹ç‚¹åˆ†æ (`analyze_vocabulary`)**
```python
# spaCyçš„è¯æ±‡åŒ–å¤„ç†
words = [token.text for token in doc if token.is_alpha and token.text.lower() not in self.stop_words]

# å­¦æœ¯è¯æ±‡è¯†åˆ«ï¼ˆåŸºäºè¯ç¼€æ¨¡å¼ï¼‰
academic_patterns = [
    r'.*tion$', r'.*sion$', r'.*ment$',  # åè¯åç¼€
    r'^analy.*', r'^investig.*', r'^examin.*',  # ç ”ç©¶åŠ¨è¯
    r'^signific.*', r'^substantial.*', r'^considerable.*'  # ç¨‹åº¦è¯
]
```

**å…³é”®æŒ‡æ ‡ï¼š**
- `total_words`: æ€»è¯æ•°ï¼ˆç»è¿‡spaCyè¿‡æ»¤ï¼‰
- `unique_words`: å”¯ä¸€è¯æ•°
- `vocabulary_richness`: è¯æ±‡ä¸°å¯Œåº¦ï¼ˆType-Token Ratioï¼‰
- `academic_word_ratio`: å­¦æœ¯è¯æ±‡æ¯”ä¾‹
- `most_common_words`: é«˜é¢‘è¯ç»Ÿè®¡

**C. æ®µè½ç»“æ„åˆ†æ (`analyze_paragraph_structure`)**
```python
# æ®µè½åˆ†å‰²
paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]

# ä½¿ç”¨spaCyåˆ†ææ®µè½é•¿åº¦
paragraph_lengths = []
for para in paragraphs:
    para_doc = self.nlp(para)
    paragraph_lengths.append(len([token for token in para_doc if token.is_alpha]))
```

**å…³é”®æŒ‡æ ‡ï¼š**
- `total_paragraphs`: æ€»æ®µè½æ•°
- `avg_paragraph_length`: å¹³å‡æ®µè½é•¿åº¦
- `paragraph_length_variance`: æ®µè½é•¿åº¦å˜åŒ–
- `topic_sentence_analysis`: ä¸»é¢˜å¥ç‰¹å¾åˆ†æ

**D. å­¦æœ¯è¡¨è¾¾ä¹ æƒ¯åˆ†æ (`analyze_academic_expression`)**
```python
# spaCyçš„è¢«åŠ¨è¯­æ€æ£€æµ‹
def _calculate_passive_voice_ratio(self, sentences: List[str]) -> float:
    total_verbs = 0
    passive_verbs = 0
    
    for sentence in sentences:
        sent_doc = self.nlp(sentence)
        for token in sent_doc:
            if token.pos_ == "AUX" and token.dep_ == "auxpass":
                passive_verbs += 1
            elif token.pos_ == "VERB":
                total_verbs += 1
```

**å…³é”®æŒ‡æ ‡ï¼š**
- `passive_voice_ratio`: è¢«åŠ¨è¯­æ€æ¯”ä¾‹ï¼ˆåŸºäºspaCyè¯­æ³•åˆ†æï¼‰
- `first_person_usage`: ç¬¬ä¸€äººç§°ä½¿ç”¨æƒ…å†µ
- `qualifier_usage`: é™å®šè¯ä½¿ç”¨æƒ…å†µ

#### 3. spaCyé«˜çº§åŠŸèƒ½åº”ç”¨

**A. å‘½åå®ä½“è¯†åˆ«**
```python
def analyze_advanced_features(self, text: str) -> Dict:
    doc = self.nlp(text)
    
    # è¯†åˆ«å­¦æœ¯æ–‡æœ¬ä¸­çš„å®ä½“
    entities = {}
    for ent in doc.ents:
        if ent.label_ not in entities:
            entities[ent.label_] = []
        entities[ent.label_].append(ent.text)
```

**B. è¯æ€§æ ‡æ³¨ä¸ä¾å­˜åˆ†æ**
```python
# è¯æ€§åˆ†å¸ƒç»Ÿè®¡
pos_counts = {}
for token in doc:
    if token.pos_ not in pos_counts:
        pos_counts[token.pos_] = 0
    pos_counts[token.pos_] += 1

# ä¾å­˜å…³ç³»æ¨¡å¼
dependency_patterns = {}
for token in doc:
    if token.dep_ not in dependency_patterns:
        dependency_patterns[token.dep_] = 0
    dependency_patterns[token.dep_] += 1
```

**C. è¯å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦**
```python
def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
    doc1 = self.nlp(text1)
    doc2 = self.nlp(text2)
    # ä½¿ç”¨spaCyçš„å†…ç½®ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäº685kè¯å‘é‡ï¼‰
    similarity = doc1.similarity(doc2)
    return float(similarity)
```

**D. å­¦æœ¯å…³é”®è¯æå–**
```python
def extract_academic_keywords(self, text: str, top_n: int = 10) -> List[Dict]:
    doc = self.nlp(text)
    word_scores = {}
    
    for token in doc:
        if (token.is_alpha and 
            token.pos_ in ['NOUN', 'ADJ', 'VERB'] and 
            len(token.text) > 3 and 
            token.text.lower() not in self.stop_words):
            
            word = token.text.lower()
            base_score = word_scores.get(word, 0) + 1
            
            # è¯æ€§æƒé‡
            pos_weight = {'NOUN': 1.5, 'ADJ': 1.2, 'VERB': 1.3}.get(token.pos_, 1.0)
            
            # å­¦æœ¯è¯æ±‡æƒé‡
            academic_weight = 1.5 if any(pattern in word for pattern in 
                ['tion', 'sion', 'ment', 'analy', 'investig', 'examin']) else 1.0
            
            word_scores[word] = base_score * pos_weight * academic_weight
```

### ğŸ¤– ç¬¬äºŒå±‚ï¼šAIæ·±åº¦åˆ†æ

#### 1. AIåˆ†æè¾“å…¥å‡†å¤‡
spaCyæå–çš„ç»“æ„åŒ–æ•°æ®è¢«æ•´åˆä¸ºAIåˆ†æçš„è¾“å…¥ï¼š

```python
# NLPåˆ†æç»“æœ
nlp_analysis = {
    'sentence_structure': self.nlp_utils.analyze_sentence_structure(paper_text),
    'vocabulary': self.nlp_utils.analyze_vocabulary(paper_text),
    'paragraph_structure': self.nlp_utils.analyze_paragraph_structure(paper_text),
    'academic_expression': self.nlp_utils.analyze_academic_expression(paper_text)
}
```

#### 2. AI Promptè®¾è®¡
AIåˆ†æåŸºäºè¯¦ç»†çš„promptæ¨¡æ¿ï¼Œè¦æ±‚åˆ†æä»¥ä¸‹ç»´åº¦ï¼š

**A. å¥å¼ç»“æ„ç‰¹å¾**
- å¹³å‡å¥å­é•¿åº¦ï¼ˆåŸºäºspaCyçš„ç²¾ç¡®ç»Ÿè®¡ï¼‰
- å¤åˆå¥æ¯”ä¾‹ï¼ˆåˆ©ç”¨spaCyè¯†åˆ«çš„è¿æ¥è¯ï¼‰
- å¤æ‚å¥æ¨¡å¼ï¼ˆåŸºäºspaCyçš„ä»å±è¿è¯è¯†åˆ«ï¼‰
- å¥é•¿å˜åŒ–ï¼ˆåˆ©ç”¨spaCyçš„æ–¹å·®è®¡ç®—ï¼‰

**B. è¯æ±‡ç‰¹å¾**
- å­¦æœ¯è¯æ±‡å¯†åº¦ï¼ˆç»“åˆspaCyçš„è¯ç¼€è¯†åˆ«ï¼‰
- ä¸“ä¸šæœ¯è¯­é¢‘ç‡
- åŠ¨è¯æ—¶æ€åå¥½ï¼ˆåŸºäºspaCyçš„è¯æ€§æ ‡æ³¨ï¼‰
- é«˜é¢‘è¯åˆ†æï¼ˆåŸºäºspaCyçš„è¯é¢‘ç»Ÿè®¡ï¼‰

**C. æ®µè½ç»„ç»‡**
- å¹³å‡æ®µè½é•¿åº¦ï¼ˆåŸºäºspaCyçš„æ®µè½åˆ†æï¼‰
- ä¸»é¢˜å¥ä½ç½®ï¼ˆç»“åˆspaCyçš„å¥å­åˆ†å‰²ï¼‰
- è®ºè¯é€»è¾‘ç»“æ„

**D. å­¦æœ¯è¡¨è¾¾ä¹ æƒ¯**
- è¢«åŠ¨è¯­æ€ä½¿ç”¨ç‡ï¼ˆåŸºäºspaCyçš„è¯­æ³•åˆ†æï¼‰
- ç¬¬ä¸€äººç§°ä½¿ç”¨ï¼ˆåˆ©ç”¨spaCyçš„ä»£è¯è¯†åˆ«ï¼‰
- é™å®šè¯ä½¿ç”¨ï¼ˆåŸºäºspaCyçš„è¯æ±‡åˆ†æï¼‰

#### 3. AIåˆ†æè¾“å‡ºæ ¼å¼
```json
{
  "sentence_structure": {
    "avg_sentence_length": 0,
    "compound_sentence_ratio": 0.5,
    "complex_sentence_ratio": 0.3,
    "sentence_length_variance": 0
  },
  "vocabulary": {
    "academic_word_ratio": 0.5,
    "professional_terminology_frequency": 0,
    "verb_tense_preference": "past/present/mixed",
    "top_words": ["word1", "word2", ...]
  },
  "paragraph_organization": {
    "avg_paragraph_length": 0,
    "topic_sentence_position": "beginning/middle/mixed",
    "argument_structure": "description"
  },
  "academic_expression": {
    "passive_voice_ratio": 0.0,
    "first_person_usage": 0.0,
    "qualifier_usage": 0.0
  },
  "citation_argument": {
    "citation_format": "APA/MLA/Other",
    "argument_pattern": "description of argument pattern"
  }
}
```

### ğŸ”— ç¬¬ä¸‰å±‚ï¼šç»“æœèåˆä¸è¾“å‡º

#### 1. æ•°æ®æ•´åˆ
```python
combined_analysis = {
    'paper_id': paper_id,
    'analysis_date': datetime.now().isoformat(),
    'nlp_analysis': nlp_analysis,      # spaCyæå–çš„é‡åŒ–æ•°æ®
    'gpt_analysis': gpt_analysis,      # AIæ·±åº¦åˆ†æç»“æœ
    'text_length': len(paper_text),
    'word_count': len(paper_text.split())
}
```

#### 2. åˆ†ææŠ¥å‘Šç»“æ„
```json
{
  "paper_id": "example_paper",
  "analysis_date": "2025-01-14T10:30:00",
  "nlp_analysis": {
    "sentence_structure": {
      "total_sentences": 150,
      "avg_sentence_length": 18.5,
      "compound_sentence_ratio": 0.35,
      "complex_sentence_ratio": 0.42,
      "sentence_length_variance": 45.2
    },
    "vocabulary": {
      "total_words": 2850,
      "unique_words": 680,
      "vocabulary_richness": 0.238,
      "academic_word_ratio": 0.156,
      "most_common_words": {
        "research": 45,
        "study": 38,
        "analysis": 32
      }
    },
    "paragraph_structure": {
      "total_paragraphs": 25,
      "avg_paragraph_length": 114.0,
      "paragraph_length_variance": 156.8
    },
    "academic_expression": {
      "passive_voice_ratio": 0.23,
      "first_person_usage": {
        "first_person_count": 3,
        "first_person_ratio": 0.001
      },
      "qualifier_usage": {
        "qualifier_count": 12,
        "qualifier_ratio": 0.004
      }
    }
  },
  "gpt_analysis": {
    "sentence_structure": {
      "avg_sentence_length": 18.5,
      "compound_sentence_ratio": 0.35,
      "complex_sentence_ratio": 0.42
    },
    "vocabulary": {
      "academic_word_ratio": 0.156,
      "professional_terminology_frequency": 0.089,
      "verb_tense_preference": "mixed",
      "top_words": ["research", "study", "analysis", "methodology"]
    },
    "paragraph_organization": {
      "avg_paragraph_length": 114.0,
      "topic_sentence_position": "beginning",
      "argument_structure": "linear progression with supporting evidence"
    },
    "academic_expression": {
      "passive_voice_ratio": 0.23,
      "first_person_usage": 0.001,
      "qualifier_usage": 0.004
    },
    "citation_argument": {
      "citation_format": "APA",
      "argument_pattern": "evidence-based with systematic presentation"
    }
  },
  "text_length": 45000,
  "word_count": 2850
}
```

### ğŸ¯ spaCyä¸AIåä½œçš„ä¼˜åŠ¿

#### 1. æ•°æ®äº’è¡¥æ€§
- **spaCyæä¾›ç²¾ç¡®çš„é‡åŒ–æ•°æ®**ï¼šå¥é•¿ã€è¯é¢‘ã€è¯­æ³•ç»“æ„ç­‰å®¢è§‚æŒ‡æ ‡
- **AIæä¾›æ·±åº¦è¯­ä¹‰åˆ†æ**ï¼šå†™ä½œæ¨¡å¼ã€è®ºè¯é€»è¾‘ã€é£æ ¼ç‰¹å¾ç­‰ä¸»è§‚åˆ¤æ–­

#### 2. åˆ†æå±‚æ¬¡æ€§
- **åŸºç¡€å±‚ï¼ˆspaCyï¼‰**ï¼šè¯­æ³•ã€è¯æ±‡ã€å¥æ³•ç»“æ„çš„ç»Ÿè®¡ç‰¹å¾
- **è¯­ä¹‰å±‚ï¼ˆAIï¼‰**ï¼šå†™ä½œæ„å›¾ã€è®ºè¯ç­–ç•¥ã€è¡¨è¾¾é£æ ¼çš„åˆ†æ

#### 3. ç»“æœéªŒè¯æ€§
- **äº¤å‰éªŒè¯**ï¼šspaCyçš„å®¢è§‚æ•°æ®ä¸AIçš„ä¸»è§‚åˆ†æç›¸äº’éªŒè¯
- **ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿åˆ†æç»“æœçš„é€»è¾‘ä¸€è‡´æ€§å’Œå¯é æ€§

#### 4. å¤„ç†æ•ˆç‡æ€§
- **å¹¶è¡Œå¤„ç†**ï¼šspaCyçš„å¿«é€Ÿé¢„å¤„ç†ä¸ºAIåˆ†ææä¾›ç»“æ„åŒ–è¾“å…¥
- **åˆ†å±‚ä¼˜åŒ–**ï¼šä¸åŒå±‚æ¬¡ä½¿ç”¨æœ€é€‚åˆçš„åˆ†æå·¥å…·å’Œå‚æ•°

### ğŸ“ˆ å®é™…åº”ç”¨æ•ˆæœ

#### åˆ†æç²¾åº¦æå‡
- **å¥æ³•åˆ†æç²¾åº¦**ï¼šspaCyçš„en_core_web_mdæ¨¡å‹æä¾›95%+çš„è¯æ€§æ ‡æ³¨å‡†ç¡®ç‡
- **è¯­ä¹‰ç†è§£æ·±åº¦**ï¼šAIæ¨¡å‹åŸºäºspaCyçš„ç»“æ„åŒ–æ•°æ®æä¾›æ›´æ·±å±‚çš„è¯­ä¹‰åˆ†æ
- **é£æ ¼è¯†åˆ«å‡†ç¡®æ€§**ï¼šç»“åˆå®¢è§‚ç»Ÿè®¡å’Œä¸»è§‚åˆ¤æ–­ï¼Œæé«˜é£æ ¼æ¨¡å¼è¯†åˆ«çš„å‡†ç¡®æ€§

#### å¤„ç†æ•ˆç‡ä¼˜åŒ–
- **é¢„å¤„ç†åŠ é€Ÿ**ï¼šspaCyçš„Cythonå®ç°æä¾›2-3å€çš„å¤„ç†é€Ÿåº¦æå‡
- **å†…å­˜ä¼˜åŒ–**ï¼šspaCyçš„æµå¼å¤„ç†å‡å°‘å†…å­˜å ç”¨
- **æ‰¹å¤„ç†æ”¯æŒ**ï¼šæ”¯æŒå¤§è§„æ¨¡æ–‡æœ¬çš„æ‰¹é‡åˆ†æ

#### ç»“æœå¯è§£é‡Šæ€§
- **é‡åŒ–æŒ‡æ ‡**ï¼šspaCyæä¾›å¯éªŒè¯çš„ç»Ÿè®¡æŒ‡æ ‡
- **å®šæ€§åˆ†æ**ï¼šAIæä¾›å¯ç†è§£çš„é£æ ¼æè¿°
- **è¯æ®æ”¯æŒ**ï¼šæ¯ä¸ªåˆ†æç»“è®ºéƒ½æœ‰å…·ä½“çš„æ•°æ®æ”¯æ’‘

è¿™ç§spaCyä¸AIçš„åˆ†å±‚åä½œæ¨¡å¼ï¼Œæ—¢ä¿è¯äº†åˆ†æçš„å®¢è§‚æ€§å’Œå‡†ç¡®æ€§ï¼Œåˆæä¾›äº†æ·±åº¦å’Œå¹¿åº¦ï¼Œæ˜¯å•ä¸ªæ–‡ä»¶åˆ†æåŠŸèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¼˜åŠ¿ã€‚

## æ€»ç»“

å•ä¸ªPDFæ–‡ä»¶åˆ†æåŠŸèƒ½æä¾›äº†çµæ´»ã€å¯æ§çš„è®ºæ–‡åˆ†ææ–¹å¼ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦ç²¾ç»†æ§åˆ¶åˆ†æè¿‡ç¨‹çš„åœºæ™¯ã€‚é€šè¿‡spaCyä¸AIçš„æ·±åº¦åä½œï¼Œç³»ç»Ÿèƒ½å¤Ÿæä¾›æ—¢ç²¾ç¡®åˆå…¨é¢çš„æ–‡æœ¬åˆ†æï¼Œä¸ºå­¦æœ¯å†™ä½œæä¾›ç§‘å­¦çš„æ•°æ®æ”¯æ’‘ã€‚é€šè¿‡åˆç†çš„é…ç½®å’Œä½¿ç”¨ï¼Œå¯ä»¥é«˜æ•ˆåœ°å®Œæˆå¤§é‡è®ºæ–‡çš„åˆ†æå·¥ä½œã€‚
