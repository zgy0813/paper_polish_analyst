# NLPåˆ†æåŸç†è¯¦è§£æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†é˜è¿°äº†è®ºæ–‡é£æ ¼åˆ†æä¸æ¶¦è‰²ç³»ç»Ÿä¸­NLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰åˆ†æçš„åŸç†ã€æ–¹æ³•å’Œå®ç°ç»†èŠ‚ã€‚NLPåˆ†ææ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼Œè´Ÿè´£ä»æ–‡æœ¬ä¸­æå–è¯­è¨€ç‰¹å¾ã€ç»Ÿè®¡æŒ‡æ ‡å’Œé£æ ¼æ¨¡å¼ï¼Œä¸ºåç»­çš„AIåˆ†æå’Œé£æ ¼æŒ‡å—ç”Ÿæˆæä¾›æ•°æ®åŸºç¡€ã€‚

## ğŸ¯ NLPåˆ†æåœ¨ç³»ç»Ÿä¸­çš„ä½œç”¨

### åˆ†æå±‚æ¬¡æ¶æ„
```
PDFæ–‡æœ¬ â†’ NLPé¢„å¤„ç† â†’ ç‰¹å¾æå– â†’ ç»Ÿè®¡åˆ†æ â†’ æ¨¡å¼è¯†åˆ« â†’ AIæ·±åº¦åˆ†æ
```

### æ ¸å¿ƒåŠŸèƒ½å®šä½
- **æ•°æ®é¢„å¤„ç†**ï¼šæ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€å¥æ³•åˆ†æ
- **ç‰¹å¾å·¥ç¨‹**ï¼šæå–å¯é‡åŒ–çš„è¯­è¨€ç‰¹å¾
- **ç»Ÿè®¡åˆ†æ**ï¼šè®¡ç®—è¯­è¨€ä½¿ç”¨æ¨¡å¼å’Œé¢‘ç‡
- **è´¨é‡è¯„ä¼°**ï¼šå¯è¯»æ€§ã€å¤æ‚åº¦ã€å­¦æœ¯è§„èŒƒæ€§è¯„ä¼°
- **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šæ–‡æœ¬é—´ç›¸ä¼¼æ€§åº¦é‡å’Œèšç±»

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯æ ˆ

### ä¸»è¦ä¾èµ–åº“
```python
import nltk                    # è‡ªç„¶è¯­è¨€å¤„ç†æ ¸å¿ƒåº“
from nltk.tokenize import sent_tokenize, word_tokenize  # åˆ†è¯å·¥å…·
from nltk.corpus import stopwords                       # åœç”¨è¯åº“
from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFå‘é‡åŒ–
from sklearn.metrics.pairwise import cosine_similarity  # ä½™å¼¦ç›¸ä¼¼åº¦
import numpy as np             # æ•°å€¼è®¡ç®—
from collections import Counter # è¯é¢‘ç»Ÿè®¡
```

### æŠ€æœ¯é€‰æ‹©ç†ç”±
- **NLTK**ï¼šæˆç†Ÿçš„NLPåº“ï¼Œæä¾›å®Œæ•´çš„æ–‡æœ¬å¤„ç†å·¥å…·é“¾
- **scikit-learn**ï¼šå¼ºå¤§çš„æœºå™¨å­¦ä¹ åº“ï¼Œç”¨äºæ–‡æœ¬å‘é‡åŒ–å’Œç›¸ä¼¼åº¦è®¡ç®—
- **NumPy**ï¼šé«˜æ•ˆçš„æ•°å€¼è®¡ç®—ï¼Œæ”¯æŒç»Ÿè®¡åˆ†æ
- **Counter**ï¼šPythonå†…ç½®çš„é«˜æ•ˆè¯é¢‘ç»Ÿè®¡å·¥å…·

## ğŸ“Š æ ¸å¿ƒåˆ†ææ¨¡å—è¯¦è§£

### 1. å¥å¼ç»“æ„åˆ†æ (`analyze_sentence_structure`)

#### åˆ†æç›®æ ‡
è¯†åˆ«å’Œé‡åŒ–æ–‡æœ¬çš„å¥å¼ç‰¹å¾ï¼ŒåŒ…æ‹¬å¥å­å¤æ‚åº¦ã€é•¿åº¦åˆ†å¸ƒå’Œè¯­æ³•ç»“æ„ã€‚

#### æ ¸å¿ƒæŒ‡æ ‡

**å¥å­é•¿åº¦ç»Ÿè®¡**
```python
# è®¡ç®—å¥å­é•¿åº¦åˆ†å¸ƒ
sentence_lengths = [len(word_tokenize(sent)) for sent in sentences]
avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)
sentence_length_variance = np.var(sentence_lengths)
```

**å¤åˆå¥è¯†åˆ«**
```python
# æ£€æµ‹åŒ…å«è¿æ¥è¯çš„å¤åˆå¥
compound_indicators = ['and', 'but', 'or', 'so', 'yet', 'for', 'nor']
compound_sentences = sum(1 for sent in sentences 
                        if any(connector in sent.lower() for connector in compound_indicators))
```

**ä»å¥è¯†åˆ«**
```python
# æ£€æµ‹åŒ…å«ä»å±è¿è¯çš„å¤æ‚å¥
subordinating_conjunctions = ['because', 'although', 'while', 'since', 'if', 'when', 'where', 'which', 'that', 'who']
complex_sentences = sum(1 for sent in sentences 
                       if any(conj in sent.lower() for conj in subordinating_conjunctions))
```

#### å­¦æœ¯æ„ä¹‰
- **å¹³å‡å¥é•¿**ï¼šåæ˜ å†™ä½œçš„å¤æ‚åº¦å’Œå¯è¯»æ€§
- **å¤åˆå¥æ¯”ä¾‹**ï¼šä½“ç°è®ºè¯çš„å¤æ‚æ€§å’Œé€»è¾‘å…³ç³»
- **ä»å¥æ¯”ä¾‹**ï¼šæ˜¾ç¤ºè®ºè¿°çš„æ·±åº¦å’Œå±‚æ¬¡æ€§
- **é•¿åº¦æ–¹å·®**ï¼šè¡¨æ˜å¥å¼å˜åŒ–çš„ä¸€è‡´æ€§

### 2. è¯æ±‡ç‰¹ç‚¹åˆ†æ (`analyze_vocabulary`)

#### åˆ†æç›®æ ‡
æ·±å…¥åˆ†ææ–‡æœ¬çš„è¯æ±‡ä½¿ç”¨æ¨¡å¼ï¼ŒåŒ…æ‹¬è¯æ±‡ä¸°å¯Œåº¦ã€å­¦æœ¯è¯æ±‡æ¯”ä¾‹å’Œè¯é¢‘åˆ†å¸ƒã€‚

#### æ ¸å¿ƒæŒ‡æ ‡

**è¯æ±‡ä¸°å¯Œåº¦è®¡ç®—**
```python
# è¯æ±‡å¤šæ ·æ€§æŒ‡æ ‡ï¼ˆType-Token Ratioï¼‰
vocabulary_richness = len(word_counts) / len(words)
```

**å­¦æœ¯è¯æ±‡è¯†åˆ«**
```python
def _identify_academic_words(self, words: List[str]) -> List[str]:
    """åŸºäºè¯ç¼€æ¨¡å¼è¯†åˆ«å­¦æœ¯è¯æ±‡"""
    academic_patterns = [
        r'.*tion$', r'.*sion$', r'.*ment$',  # åè¯åç¼€
        r'^analy.*', r'^investig.*', r'^examin.*',  # ç ”ç©¶åŠ¨è¯
        r'^signific.*', r'^substantial.*', r'^considerable.*'  # ç¨‹åº¦è¯
    ]
```

**åŠ¨è¯æ—¶æ€åˆ†æ**
```python
def _analyze_verb_tenses(self, words: List[str]) -> Dict:
    """åˆ†æåŠ¨è¯æ—¶æ€åˆ†å¸ƒ"""
    past_tense_words = ['was', 'were', 'had', 'did', 'went', 'said', 'found', 'showed']
    present_tense_words = ['is', 'are', 'has', 'have', 'do', 'go', 'say', 'find', 'show']
```

#### å­¦æœ¯æ„ä¹‰
- **è¯æ±‡ä¸°å¯Œåº¦**ï¼šåæ˜ ä½œè€…çš„è¯æ±‡æŒæ¡æ°´å¹³å’Œè¡¨è¾¾å¤šæ ·æ€§
- **å­¦æœ¯è¯æ±‡æ¯”ä¾‹**ï¼šä½“ç°æ–‡æœ¬çš„å­¦æœ¯æ€§å’Œä¸“ä¸šæ€§
- **è¯é¢‘åˆ†å¸ƒ**ï¼šæ­ç¤ºå¸¸ç”¨è¯æ±‡å’Œå…³é”®æ¦‚å¿µ
- **æ—¶æ€åˆ†å¸ƒ**ï¼šåæ˜ è®ºè¿°çš„æ—¶é—´ç»´åº¦å’Œç ”ç©¶è§†è§’

### 3. æ®µè½ç»“æ„åˆ†æ (`analyze_paragraph_structure`)

#### åˆ†æç›®æ ‡
è¯„ä¼°æ–‡æœ¬çš„æ®µè½ç»„ç»‡ç»“æ„å’Œä¸»é¢˜å¥ç‰¹å¾ã€‚

#### æ ¸å¿ƒæŒ‡æ ‡

**æ®µè½é•¿åº¦ç»Ÿè®¡**
```python
paragraph_lengths = [len(word_tokenize(p)) for p in paragraphs]
avg_paragraph_length = sum(paragraph_lengths) / len(paragraph_lengths)
```

**ä¸»é¢˜å¥åˆ†æ**
```python
def _analyze_topic_sentences(self, paragraphs: List[str]) -> Dict:
    """åˆ†ææ®µè½ä¸»é¢˜å¥ç‰¹å¾"""
    topic_sentences = [sent_tokenize(para)[0] for para in paragraphs if sent_tokenize(para)]
    topic_lengths = [len(word_tokenize(ts)) for ts in topic_sentences]
```

#### å­¦æœ¯æ„ä¹‰
- **æ®µè½é•¿åº¦**ï¼šåæ˜ è®ºè¿°çš„è¯¦ç»†ç¨‹åº¦å’Œé€»è¾‘å±•å¼€
- **ä¸»é¢˜å¥ç‰¹å¾**ï¼šä½“ç°æ®µè½ç»„ç»‡çš„ä¸€è‡´æ€§å’Œæ¸…æ™°åº¦
- **æ®µè½å˜åŒ–**ï¼šæ˜¾ç¤ºè®ºè¿°èŠ‚å¥å’Œé‡ç‚¹åˆ†å¸ƒ

### 4. å­¦æœ¯è¡¨è¾¾ä¹ æƒ¯åˆ†æ (`analyze_academic_expression`)

#### åˆ†æç›®æ ‡
è¯†åˆ«å­¦æœ¯å†™ä½œä¸­çš„ç‰¹å®šè¡¨è¾¾æ¨¡å¼å’Œè¯­è¨€ä¹ æƒ¯ã€‚

#### æ ¸å¿ƒæŒ‡æ ‡

**è¢«åŠ¨è¯­æ€æ£€æµ‹**
```python
def _calculate_passive_voice_ratio(self, sentences: List[str]) -> float:
    """è®¡ç®—è¢«åŠ¨è¯­æ€æ¯”ä¾‹"""
    passive_indicators = ['was', 'were', 'been', 'being', 'is', 'are']
    # æ£€æµ‹æ¨¡å¼ï¼šbeåŠ¨è¯ + è¿‡å»åˆ†è¯
    for i, word in enumerate(words):
        if word in passive_indicators and i + 1 < len(words):
            next_word = words[i + 1]
            if any(next_word.endswith(suffix) for suffix in ['ed', 'en', 't']):
                passive_verbs += 1
```

**ç¬¬ä¸€äººç§°ä½¿ç”¨åˆ†æ**
```python
def _analyze_first_person_usage(self, words: List[str]) -> Dict:
    """åˆ†æç¬¬ä¸€äººç§°ä»£è¯ä½¿ç”¨æƒ…å†µ"""
    first_person_words = ['i', 'we', 'my', 'our', 'me', 'us']
    first_person_count = sum(words.count(word) for word in first_person_words)
```

**é™å®šè¯ä½¿ç”¨åˆ†æ**
```python
def _analyze_qualifiers(self, words: List[str]) -> Dict:
    """åˆ†æé™å®šè¯å’Œä¿®é¥°è¯­ä½¿ç”¨"""
    qualifiers = ['very', 'quite', 'rather', 'somewhat', 'fairly', 'relatively', 'considerably']
    qualifier_count = sum(words.count(word) for word in qualifiers)
```

#### å­¦æœ¯æ„ä¹‰
- **è¢«åŠ¨è¯­æ€æ¯”ä¾‹**ï¼šä½“ç°å®¢è§‚æ€§å’Œæ­£å¼æ€§ç¨‹åº¦
- **ç¬¬ä¸€äººç§°ä½¿ç”¨**ï¼šåæ˜ ä½œè€…å‚ä¸åº¦å’Œä¸»è§‚æ€§
- **é™å®šè¯ä½¿ç”¨**ï¼šæ˜¾ç¤ºè¡¨è¾¾çš„è°¨æ…æ€§å’Œå‡†ç¡®æ€§

### 5. æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®— (`calculate_text_similarity`)

#### æŠ€æœ¯åŸç†
ä½¿ç”¨TF-IDFï¼ˆTerm Frequency-Inverse Document Frequencyï¼‰å‘é‡åŒ–å’Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ã€‚

#### å®ç°æ­¥éª¤

**TF-IDFå‘é‡åŒ–**
```python
# åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
tfidf_matrix = vectorizer.fit_transform([text1, text2])
```

**ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—**
```python
# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
```

#### å­¦æœ¯æ„ä¹‰
- **å†…å®¹ç›¸ä¼¼æ€§**ï¼šè¯†åˆ«ç›¸å…³ç ”ç©¶å’Œé‡å¤å†…å®¹
- **é£æ ¼ä¸€è‡´æ€§**ï¼šæ£€æµ‹æ–‡æœ¬é£æ ¼çš„å˜åŒ–
- **èšç±»åˆ†æ**ï¼šä¸ºè®ºæ–‡åˆ†ç»„å’Œæ¨¡å¼è¯†åˆ«æä¾›åŸºç¡€

### 6. å¯è¯»æ€§è¯„åˆ† (`calculate_readability_score`)

#### è¯„åˆ†ç»´åº¦

**å¥é•¿å› å­**
```python
# å¥å­é•¿åº¦å¯¹å¯è¯»æ€§çš„å½±å“
avg_sentence_length = len(words) / len(sentences)
sentence_factor = (1 / (1 + avg_sentence_length / 20)) * 40
```

**è¯æ±‡ä¸°å¯Œåº¦**
```python
# è¯æ±‡å¤šæ ·æ€§è¯„åˆ†
unique_words = len(set(word.lower() for word in words if word.isalpha()))
vocabulary_richness = unique_words / len(words)
vocabulary_factor = vocabulary_richness * 30
```

**è¿æ¥è¯å¯†åº¦**
```python
# è¿æ¥è¯ä½¿ç”¨å¯¹æµç•…æ€§çš„å½±å“
connecting_words = ['and', 'but', 'or', 'so', 'yet', 'for', 'nor', 'however', 'therefore', 'moreover']
connecting_word_count = sum(words.count(word) for word in connecting_words)
connection_density = connecting_word_count / len(words)
connection_factor = min(connection_density * 100, 30)
```

**ç»¼åˆè¯„åˆ†**
```python
readability = sentence_factor + vocabulary_factor + connection_factor
return min(max(readability, 0), 100)
```

## ğŸ”¬ ç®—æ³•åŸç†æ·±åº¦è§£æ

### TF-IDFç®—æ³•åŸç†

#### æ•°å­¦å…¬å¼
```
TF(t,d) = f(t,d) / Î£f(w,d)  # è¯é¢‘
IDF(t,D) = log(|D| / |{dâˆˆD : tâˆˆd}|)  # é€†æ–‡æ¡£é¢‘ç‡
TF-IDF(t,d,D) = TF(t,d) Ã— IDF(t,D)
```

#### åœ¨ç³»ç»Ÿä¸­çš„åº”ç”¨
- **ç‰¹å¾æå–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡
- **é‡è¦æ€§æƒé‡**ï¼šçªå‡ºå…³é”®è¯æ±‡ï¼ŒæŠ‘åˆ¶å¸¸è§è¯æ±‡
- **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šä¸ºæ–‡æœ¬æ¯”è¾ƒæä¾›é‡åŒ–åŸºç¡€

### ä½™å¼¦ç›¸ä¼¼åº¦åŸç†

#### æ•°å­¦å…¬å¼
```
cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
```

#### ä¼˜åŠ¿
- **æ ‡å‡†åŒ–**ï¼šä¸å—å‘é‡é•¿åº¦å½±å“
- **èŒƒå›´å›ºå®š**ï¼šç»“æœåœ¨[-1, 1]ä¹‹é—´
- **ç›´è§‚æ€§**ï¼š1è¡¨ç¤ºå®Œå…¨ç›¸ä¼¼ï¼Œ0è¡¨ç¤ºæ— å…³

### ç»Ÿè®¡åˆ†ææ–¹æ³•

#### æè¿°æ€§ç»Ÿè®¡
```python
# ä¸­å¿ƒè¶‹åŠ¿
mean = sum(values) / len(values)
median = sorted(values)[len(values)//2]

# ç¦»æ•£ç¨‹åº¦
variance = np.var(values)
std_dev = np.sqrt(variance)
```

#### åˆ†å¸ƒåˆ†æ
```python
# æ­£æ€æ€§æ£€éªŒï¼ˆç®€åŒ–ç‰ˆï¼‰
def check_distribution(values):
    mean_val = np.mean(values)
    std_val = np.std(values)
    # 68-95-99.7è§„åˆ™æ£€éªŒ
    within_1_std = sum(1 for v in values if abs(v - mean_val) <= std_val)
    return within_1_std / len(values) > 0.68
```

## ğŸ¨ é£æ ¼ç‰¹å¾æå–ç­–ç•¥

### 1. å¤šå±‚æ¬¡ç‰¹å¾ä½“ç³»

#### è¯­æ³•å±‚ç‰¹å¾
- **å¥æ³•å¤æ‚åº¦**ï¼šä»å¥æ•°é‡ã€åµŒå¥—æ·±åº¦
- **å¥å‹å˜åŒ–**ï¼šé™ˆè¿°å¥ã€ç–‘é—®å¥ã€æ„Ÿå¹å¥æ¯”ä¾‹
- **è¯­æ³•æ­£ç¡®æ€§**ï¼šè¯­æ³•é”™è¯¯æ£€æµ‹å’Œç»Ÿè®¡

#### è¯­ä¹‰å±‚ç‰¹å¾
- **è¯æ±‡é€‰æ‹©**ï¼šåŒä¹‰è¯ä½¿ç”¨åå¥½
- **è¡¨è¾¾æ–¹å¼**ï¼šç›´æ¥è¡¨è¾¾vsé—´æ¥è¡¨è¾¾
- **é€»è¾‘å…³ç³»**ï¼šå› æœå…³ç³»ã€å¯¹æ¯”å…³ç³»ã€é€’è¿›å…³ç³»

#### è¯­ç”¨å±‚ç‰¹å¾
- **æ­£å¼ç¨‹åº¦**ï¼šæ­£å¼è¯æ±‡vséæ­£å¼è¯æ±‡æ¯”ä¾‹
- **å®¢è§‚æ€§**ï¼šä¸»è§‚è¡¨è¾¾vså®¢è§‚è¡¨è¾¾
- **æƒå¨æ€§**ï¼šå¼•ç”¨å’Œè¯æ®ä½¿ç”¨æ¨¡å¼

### 2. æ¨¡å¼è¯†åˆ«ç®—æ³•

#### é¢‘ç¹æ¨¡å¼æŒ–æ˜
```python
def find_frequent_patterns(patterns, min_support=0.1):
    """å‘ç°é¢‘ç¹å‡ºç°çš„è¯­è¨€æ¨¡å¼"""
    pattern_counts = Counter(patterns)
    total_patterns = len(patterns)
    
    frequent_patterns = {
        pattern: count / total_patterns 
        for pattern, count in pattern_counts.items()
        if count / total_patterns >= min_support
    }
    
    return frequent_patterns
```

#### å¼‚å¸¸æ£€æµ‹
```python
def detect_anomalies(values, threshold=2):
    """æ£€æµ‹å¼‚å¸¸å€¼"""
    mean_val = np.mean(values)
    std_val = np.std(values)
    
    anomalies = [
        (i, val) for i, val in enumerate(values)
        if abs(val - mean_val) > threshold * std_val
    ]
    
    return anomalies
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. è®¡ç®—æ•ˆç‡ä¼˜åŒ–

#### å‘é‡åŒ–è®¡ç®—
```python
# ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
sentence_lengths = np.array([len(sent.split()) for sent in sentences])
avg_length = np.mean(sentence_lengths)
variance = np.var(sentence_lengths)
```

#### ç¼“å­˜æœºåˆ¶
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def cached_tokenize(text):
    """ç¼“å­˜åˆ†è¯ç»“æœ"""
    return word_tokenize(text)
```

#### æ‰¹é‡å¤„ç†
```python
def batch_process_texts(texts, batch_size=100):
    """æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œå‡å°‘é‡å¤åˆå§‹åŒ–å¼€é”€"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_results = process_batch(batch)
        results.extend(batch_results)
    return results
```

### 2. å†…å­˜ä¼˜åŒ–

#### ç”Ÿæˆå™¨ä½¿ç”¨
```python
def tokenize_large_text(text):
    """ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ–‡æœ¬"""
    for sentence in sent_tokenize(text):
        yield word_tokenize(sentence)
```

#### ç¨€ç–çŸ©é˜µ
```python
from scipy.sparse import csr_matrix

# ä½¿ç”¨ç¨€ç–çŸ©é˜µå­˜å‚¨TF-IDFç»“æœ
tfidf_sparse = csr_matrix(tfidf_matrix)
```

### 3. ç²¾åº¦ä¼˜åŒ–

#### é¢„å¤„ç†ä¼˜åŒ–
```python
def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†ä¼˜åŒ–"""
    # ç»Ÿä¸€ç¼–ç 
    text = text.encode('utf-8', errors='ignore').decode('utf-8')
    
    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\.\,\!\?\;\:]', ' ', text)
    
    # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

#### é”™è¯¯å¤„ç†
```python
def robust_tokenize(text):
    """å¥å£®çš„åˆ†è¯å¤„ç†"""
    try:
        return word_tokenize(text)
    except Exception as e:
        # å›é€€åˆ°ç®€å•åˆ†è¯
        return text.split()
```

## ğŸ” è´¨é‡è¯„ä¼°ä½“ç³»

### 1. è¯„ä¼°ç»´åº¦

#### å‡†ç¡®æ€§è¯„ä¼°
- **è¯­æ³•æ­£ç¡®æ€§**ï¼šè¯­æ³•é”™è¯¯æ£€æµ‹å’Œç»Ÿè®¡
- **æ‹¼å†™æ­£ç¡®æ€§**ï¼šæ‹¼å†™é”™è¯¯è¯†åˆ«
- **è¯­ä¹‰ä¸€è‡´æ€§**ï¼šå‰åæ–‡è¯­ä¹‰è¿è´¯æ€§

#### å®Œæ•´æ€§è¯„ä¼°
- **ä¿¡æ¯å®Œæ•´æ€§**ï¼šå…³é”®ä¿¡æ¯ç¼ºå¤±æ£€æµ‹
- **ç»“æ„å®Œæ•´æ€§**ï¼šå¿…è¦ç»“æ„å…ƒç´ æ£€æŸ¥
- **é€»è¾‘å®Œæ•´æ€§**ï¼šè®ºè¯é€»è¾‘é“¾å®Œæ•´æ€§

#### ä¸€è‡´æ€§è¯„ä¼°
- **æœ¯è¯­ä¸€è‡´æ€§**ï¼šä¸“ä¸šæœ¯è¯­ä½¿ç”¨ä¸€è‡´æ€§
- **æ ¼å¼ä¸€è‡´æ€§**ï¼šå¼•ç”¨æ ¼å¼ã€æ ‡ç‚¹ä½¿ç”¨ä¸€è‡´æ€§
- **é£æ ¼ä¸€è‡´æ€§**ï¼šæ•´ä½“é£æ ¼ç»Ÿä¸€æ€§

### 2. è¯„åˆ†ç®—æ³•

#### åŠ æƒè¯„åˆ†æ¨¡å‹
```python
def calculate_quality_score(text_metrics):
    """è®¡ç®—æ–‡æœ¬è´¨é‡è¯„åˆ†"""
    weights = {
        'readability': 0.3,
        'academic_style': 0.25,
        'grammar_accuracy': 0.2,
        'vocabulary_richness': 0.15,
        'consistency': 0.1
    }
    
    score = sum(metrics[dimension] * weight 
               for dimension, weight in weights.items()
               if dimension in metrics)
    
    return min(max(score, 0), 100)
```

#### åŠ¨æ€æƒé‡è°ƒæ•´
```python
def adjust_weights_by_domain(text_type):
    """æ ¹æ®æ–‡æœ¬ç±»å‹è°ƒæ•´æƒé‡"""
    if text_type == 'academic_paper':
        return {
            'academic_style': 0.4,
            'readability': 0.2,
            'grammar_accuracy': 0.2,
            'vocabulary_richness': 0.15,
            'consistency': 0.05
        }
    elif text_type == 'general_writing':
        return {
            'readability': 0.4,
            'grammar_accuracy': 0.3,
            'vocabulary_richness': 0.2,
            'consistency': 0.1
        }
```

## ğŸš€ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå­¦æœ¯è®ºæ–‡é£æ ¼åˆ†æ

#### è¾“å…¥æ–‡æœ¬
```
This study investigates the relationship between organizational culture and employee performance. 
The research methodology employs a mixed-methods approach, combining quantitative surveys with 
qualitative interviews. The findings demonstrate significant correlations between cultural 
dimensions and performance metrics.
```

#### NLPåˆ†æç»“æœ
```json
{
  "sentence_structure": {
    "total_sentences": 3,
    "avg_sentence_length": 18.7,
    "compound_sentence_ratio": 0.33,
    "complex_sentence_ratio": 0.67
  },
  "vocabulary": {
    "total_words": 56,
    "unique_words": 52,
    "vocabulary_richness": 0.93,
    "academic_word_ratio": 0.25
  },
  "academic_expression": {
    "passive_voice_ratio": 0.18,
    "first_person_usage": 0.0,
    "qualifier_usage": 0.02
  },
  "readability_score": 78.5
}
```

#### åˆ†æè§£è¯»
- **é«˜å­¦æœ¯è¯æ±‡æ¯”ä¾‹**ï¼š25%çš„å­¦æœ¯è¯æ±‡ä½“ç°ä¸“ä¸šæ€§
- **é€‚ä¸­çš„è¢«åŠ¨è¯­æ€**ï¼š18%çš„è¢«åŠ¨è¯­æ€ä¿æŒå®¢è§‚æ€§
- **æ— ç¬¬ä¸€äººç§°**ï¼šç¬¦åˆå­¦æœ¯å†™ä½œè§„èŒƒ
- **è‰¯å¥½å¯è¯»æ€§**ï¼š78.5åˆ†è¡¨æ˜è¡¨è¾¾æ¸…æ™°

### æ¡ˆä¾‹2ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—

#### æ–‡æœ¬å¯¹æ¯”è¾ƒ
```
æ–‡æœ¬A: "The research methodology involves quantitative analysis of survey data."
æ–‡æœ¬B: "The study employs statistical methods to examine questionnaire responses."
```

#### ç›¸ä¼¼åº¦è®¡ç®—è¿‡ç¨‹
1. **é¢„å¤„ç†**ï¼šåˆ†è¯ã€å»åœç”¨è¯
2. **TF-IDFå‘é‡åŒ–**ï¼šè½¬æ¢ä¸ºæ•°å€¼å‘é‡
3. **ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—**ï¼šå¾—åˆ°ç›¸ä¼¼åº¦åˆ†æ•°
4. **ç»“æœ**ï¼šç›¸ä¼¼åº¦ = 0.73

#### åº”ç”¨åœºæ™¯
- **é‡å¤å†…å®¹æ£€æµ‹**ï¼šè¯†åˆ«ç›¸ä¼¼çš„è®ºè¿°æ®µè½
- **å¼•ç”¨å…³ç³»åˆ†æ**ï¼šå‘ç°æ–‡çŒ®é—´çš„å…³è”æ€§
- **é£æ ¼ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿æ•´ä½“é£æ ¼ç»Ÿä¸€

## ğŸ”§ ç³»ç»Ÿé›†æˆä¸æ‰©å±•

### 1. æ¨¡å—åŒ–è®¾è®¡

#### æ¥å£æ ‡å‡†åŒ–
```python
class NLPAnalyzer:
    """æ ‡å‡†åŒ–çš„NLPåˆ†ææ¥å£"""
    
    def analyze(self, text: str) -> Dict:
        """ç»Ÿä¸€çš„åˆ†ææ¥å£"""
        return {
            'sentence_structure': self.analyze_sentence_structure(text),
            'vocabulary': self.analyze_vocabulary(text),
            'paragraph_structure': self.analyze_paragraph_structure(text),
            'academic_expression': self.analyze_academic_expression(text),
            'readability': self.calculate_readability_score(text)
        }
```

#### æ’ä»¶åŒ–æ‰©å±•
```python
class CustomAnalyzer(NLPAnalyzer):
    """è‡ªå®šä¹‰åˆ†æå™¨æ‰©å±•"""
    
    def analyze_custom_metric(self, text: str) -> Dict:
        """è‡ªå®šä¹‰åˆ†ææŒ‡æ ‡"""
        # å®ç°ç‰¹å®šçš„åˆ†æé€»è¾‘
        pass
    
    def analyze(self, text: str) -> Dict:
        """æ‰©å±•æ ‡å‡†åˆ†ææ¥å£"""
        base_result = super().analyze(text)
        custom_result = self.analyze_custom_metric(text)
        base_result.update(custom_result)
        return base_result
```

### 2. é…ç½®ç®¡ç†

#### å‚æ•°é…ç½®
```python
class NLPAnalysisConfig:
    """NLPåˆ†æé…ç½®ç±»"""
    
    def __init__(self):
        self.stop_words = 'english'
        self.max_features = 1000
        self.min_support = 0.1
        self.readability_weights = {
            'sentence_length': 0.4,
            'vocabulary_richness': 0.3,
            'connection_density': 0.3
        }
```

#### åŠ¨æ€é…ç½®
```python
def load_config_from_file(config_path: str) -> NLPAnalysisConfig:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°"""
    with open(config_path, 'r') as f:
        config_data = json.load(f)
    
    config = NLPAnalysisConfig()
    for key, value in config_data.items():
        setattr(config, key, value)
    
    return config
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

### 1. æ€§èƒ½æŒ‡æ ‡

#### è®¡ç®—æ€§èƒ½
```python
import time
from functools import wraps

def performance_monitor(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        execution_time = end_time - start_time
        print(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
        
        return result
    return wrapper
```

#### å†…å­˜ä½¿ç”¨ç›‘æ§
```python
import psutil
import os

def monitor_memory_usage():
    """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    return {
        'rss': memory_info.rss / 1024 / 1024,  # MB
        'vms': memory_info.vms / 1024 / 1024   # MB
    }
```

### 2. ä¼˜åŒ–ç­–ç•¥

#### ç®—æ³•ä¼˜åŒ–
```python
def optimized_similarity_calculation(texts):
    """ä¼˜åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—"""
    # é¢„è®¡ç®—TF-IDFçŸ©é˜µ
    vectorizer = TfidfVectorizer(max_features=1000)
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    return similarity_matrix
```

#### ç¼“å­˜ç­–ç•¥
```python
from functools import lru_cache
import hashlib

def get_text_hash(text):
    """ç”Ÿæˆæ–‡æœ¬å“ˆå¸Œå€¼"""
    return hashlib.md5(text.encode()).hexdigest()

@lru_cache(maxsize=1000)
def cached_analysis(text_hash, text):
    """ç¼“å­˜åˆ†æç»“æœ"""
    analyzer = NLPUtils()
    return analyzer.analyze(text)
```

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘

### 1. æŠ€æœ¯å‡çº§

#### æ·±åº¦å­¦ä¹ é›†æˆ
- **é¢„è®­ç»ƒæ¨¡å‹**ï¼šé›†æˆBERTã€GPTç­‰é¢„è®­ç»ƒæ¨¡å‹
- **è¯­ä¹‰ç†è§£**ï¼šæå‡è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ç²¾åº¦
- **ä¸Šä¸‹æ–‡åˆ†æ**ï¼šå¢å¼ºä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›

#### å¤šè¯­è¨€æ”¯æŒ
- **å¤šè¯­è¨€åˆ†è¯**ï¼šæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
- **è·¨è¯­è¨€ç›¸ä¼¼åº¦**ï¼šå®ç°è·¨è¯­è¨€æ–‡æœ¬æ¯”è¾ƒ
- **è¯­è¨€ç‰¹å®šè§„åˆ™**ï¼šé’ˆå¯¹ä¸åŒè¯­è¨€çš„ç‰¹å®šåˆ†æ

### 2. åŠŸèƒ½æ‰©å±•

#### é«˜çº§ç‰¹å¾æå–
- **æƒ…æ„Ÿåˆ†æ**ï¼šè¯†åˆ«æ–‡æœ¬æƒ…æ„Ÿå€¾å‘
- **ä¸»é¢˜å»ºæ¨¡**ï¼šè‡ªåŠ¨å‘ç°æ–‡æœ¬ä¸»é¢˜
- **å®ä½“è¯†åˆ«**ï¼šè¯†åˆ«ä¸“ä¸šæœ¯è¯­å’Œå®ä½“

#### å®æ—¶åˆ†æ
- **æµå¼å¤„ç†**ï¼šæ”¯æŒå®æ—¶æ–‡æœ¬åˆ†æ
- **å¢é‡æ›´æ–°**ï¼šæ”¯æŒå¢é‡å¼ç‰¹å¾æ›´æ–°
- **åœ¨çº¿å­¦ä¹ **ï¼šåŠ¨æ€è°ƒæ•´åˆ†ææ¨¡å‹

### 3. åº”ç”¨åœºæ™¯æ‹“å±•

#### å¤šé¢†åŸŸé€‚åº”
- **é¢†åŸŸç‰¹å®šè¯å…¸**ï¼šé’ˆå¯¹ä¸åŒå­¦ç§‘çš„ä¸“ä¸šè¯æ±‡
- **é£æ ¼æ¨¡æ¿åº“**ï¼šé¢„å®šä¹‰ä¸åŒæœŸåˆŠçš„é£æ ¼æ¨¡æ¿
- **è´¨é‡åŸºå‡†**ï¼šå»ºç«‹ä¸åŒé¢†åŸŸçš„è´¨é‡è¯„ä¼°æ ‡å‡†

#### æ™ºèƒ½åŒ–å‡çº§
- **è‡ªé€‚åº”å‚æ•°**ï¼šæ ¹æ®æ–‡æœ¬ç‰¹ç‚¹è‡ªåŠ¨è°ƒæ•´å‚æ•°
- **å¼‚å¸¸æ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«å¼‚å¸¸çš„è¯­è¨€æ¨¡å¼
- **æ¨èç³»ç»Ÿ**ï¼šåŸºäºåˆ†æç»“æœæä¾›æ”¹è¿›å»ºè®®

## ğŸ“ æ€»ç»“

NLPåˆ†æä½œä¸ºè®ºæ–‡é£æ ¼åˆ†æä¸æ¶¦è‰²ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡å¤šå±‚æ¬¡çš„ç‰¹å¾æå–å’Œç»Ÿè®¡åˆ†æï¼Œä¸ºAIæ¨¡å‹æä¾›äº†ä¸°å¯Œçš„æ–‡æœ¬ç‰¹å¾æ•°æ®ã€‚ç³»ç»Ÿçš„è®¾è®¡å……åˆ†è€ƒè™‘äº†å­¦æœ¯å†™ä½œçš„ç‰¹ç‚¹ï¼Œé€šè¿‡è¯­æ³•ã€è¯­ä¹‰ã€è¯­ç”¨ä¸‰ä¸ªå±‚é¢çš„åˆ†æï¼Œå…¨é¢è¯„ä¼°æ–‡æœ¬çš„è¯­è¨€ç‰¹å¾å’Œå†™ä½œè´¨é‡ã€‚

### æ ¸å¿ƒä¼˜åŠ¿
1. **å…¨é¢æ€§**ï¼šæ¶µç›–å¥å¼ã€è¯æ±‡ã€æ®µè½ã€è¡¨è¾¾ä¹ æƒ¯ç­‰å¤šä¸ªç»´åº¦
2. **å‡†ç¡®æ€§**ï¼šåŸºäºæˆç†Ÿçš„NLPç®—æ³•å’Œç»Ÿè®¡æ–¹æ³•
3. **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡æ”¯æŒåŠŸèƒ½æ‰©å±•å’Œå®šåˆ¶
4. **é«˜æ•ˆæ€§**ï¼šä¼˜åŒ–çš„ç®—æ³•å®ç°ä¿è¯å¤„ç†æ•ˆç‡
5. **å®ç”¨æ€§**ï¼šé’ˆå¯¹å­¦æœ¯å†™ä½œåœºæ™¯çš„ä¸“é—¨ä¼˜åŒ–

### æŠ€æœ¯ä»·å€¼
- ä¸ºAIæ¨¡å‹æä¾›ç»“æ„åŒ–çš„æ–‡æœ¬ç‰¹å¾æ•°æ®
- æ”¯æŒæ–‡æœ¬è´¨é‡çš„é‡åŒ–è¯„ä¼°å’Œæ¯”è¾ƒ
- ä¸ºé£æ ¼æŒ‡å—ç”Ÿæˆæä¾›æ•°æ®åŸºç¡€
- ä¸ºæ¶¦è‰²å»ºè®®æä¾›ç§‘å­¦ä¾æ®

### åº”ç”¨å‰æ™¯
éšç€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒNLPåˆ†ææ¨¡å—å°†åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œæ™ºèƒ½åŒ–æ–¹é¢æŒç»­æå‡ï¼Œä¸ºå­¦æœ¯å†™ä½œè¾…åŠ©ç³»ç»Ÿæä¾›æ›´åŠ ç²¾å‡†å’Œæ™ºèƒ½çš„åˆ†æèƒ½åŠ›ã€‚

---

**ğŸ”§ æŠ€æœ¯å®ç°**
- æºç ä½ç½®ï¼š`src/utils/nlp_utils.py`
- æµ‹è¯•ç”¨ä¾‹ï¼š`src/utils/nlp_utils.py` ä¸­çš„ `main()` å‡½æ•°
- é…ç½®ç®¡ç†ï¼š`config.py` ä¸­çš„ç›¸å…³é…ç½®é¡¹
